livedebugging {
  enabled = true
}

// Section 1 - Infrastructure logs with alloy

// --- Collect Logs from Alloy using the logging block

logging {
  format = "logfmt" 
  level  = "debug"
  write_to = [loki.relabel.alloy_logs.receiver]
}

// --- Use loki.relabel to add labels to the Logs

loki.relabel "alloy_logs" {
   forward_to = [loki.write.mythical.receiver]

    rule {
        target_label = "group"
        replacement = "infrastructure"
    }

    rule {
        target_label = "service"
        replacement = "alloy" 
    }
}

// -- Use loki.write to export logs to Loki

loki.write "mythical" {
    endpoint {
       url = "http://loki:3100/loki/api/v1/push"
    } 
}

//Section 2 - Infrastructure metrics with Alloy

// Use the discovery.http component to discover the targets to scrape using service discovery

discovery.http "service_discovery" {
    url = "http://service-discovery/targets.json"
    refresh_interval = "2s"
}

// scrape the target metrics using the prometheus.scrape components

prometheus.scrape "infrastructure" {
    scrape_interval = "2s"
    scrape_timeout  = "2s"

    targets    = discovery.http.service_discovery.targets
    forward_to = [prometheus.remote_write.mimir.receiver]
}

// Use prometheus.remote_write component to export metrics to the locally running mimir.

prometheus.remote_write "mimir" {
   endpoint {
    url = "http://mimir:9009/api/v1/push"
   }
}

//section 3

// Expose metrics from the postgres DB Using prometheus.exporter.postgres components

prometheus.exporter.postgres "mythical" {
    data_source_names = ["postgresql://postgres:mythical@mythical-database:5432/postgres?sslmode=disable"]
}

// Scrape metrics from the postgress using the prometheus.scrape component

prometheus.scrape "postgres" {
    scrape_interval = "2s"
    scrape_timeout  = "2s"

    targets    =  prometheus.exporter.postgres.mythical.targets
    forward_to =  [prometheus.relabel.postgres.receiver]
}

// Use the prometheus.relabel to add and modify labels

prometheus.relabel "postgres" {
    forward_to =  [prometheus.remote_write.mimir.receiver]
// Export metrics to Mimir using the prometheus.remote_write component (See section: 3 for the mimir end point)

    rule {
        target_label = "group"
        replacement  = "infrastructure"
    }
    
    rule {
        target_label = "service"
        replacement  = "postgres"
    }

 //What we have: {instance="postgresql://mythical-database:5432/postgres"}
 //What we want: {instance="mythical-database:5432/postgres"}
    
    rule {
        // Replace a label's value.
        action        = "replace"

        // The label we want to replace is 'instance'.
        target_label  = "instance"

        // Look in the existing 'instance' label for anything starting with postgresql:// and capture what follows.
        source_labels = ["instance"]
        regex         = "^postgresql://(.+)"
        
        // Replace the value with just the captured part, leaving out the prefix.
        replacement   = "$1"
    }
}

//Section 4
// Scrape metrics from the Application using the prometheus.scrape component (Manually)
// prometheus.write.queue - collects metrics sent from other components into a Write-Ahead Log (WAL) and forwards them over the network to a series of user-supplied endpoints.

prometheus.scrape "mythical" {
    scrape_interval = "2s"
    scrape_timeout  = "2s"

    targets = [
        {"__address__" = "mythical-server:4000", group = "mythical", service = "mythical-server"},
        {"__address__" = "mythical-requester:4001", group = "mythical", service = "mythical-requester"}, 
    ]

//    forward_to = [prometheus.write.queue.experimental.receiver]
      forward_to = [prometheus.relabel.mission_2.receiver]
}
prometheus.relabel "mission_2" {
    forward_to = [prometheus.write.queue.experimental.receiver]

  //define a relabel rule to extract the cloud provider from the instance_id label and add it as a new label called cloud_provider
    rule {
        action        = "replace"
        target_label  = "cloud_provider"
        source_labels = ["instance_id"]
        regex         = "^(aws|gcp|azure)-.+"
        replacement   = "$1"
    }

    // drop the instance_id label from metrics
    rule {
        action  = "labeldrop"
        regex   = "instance_id"
    }
}
prometheus.write.queue "experimental" {
    endpoint "mimir" {
        url = "http://mimir:9009/api/v1/push"
    }
}

//Section 5

// Recieve spans using otelcol.receiver.otlp component

otelcol.receiver.otlp "otlp_receiver" {
    grpc {
        endpoint = "0.0.0.0:4317"
    }
    http {
        endpoint = "0.0.0.0:4318"
    }
    output {
        traces = [
            otelcol.processor.batch.default.input,
        ]
    }
}

// Batch spans using otelcol.processor.otlp component

otelcol.processor.batch "default" {
    output {
        traces = [
            otelcol.exporter.otlp.tempo.input,
            ]
    }

    send_batch_size = 1000 
	send_batch_max_size = 2000

	timeout = "2s"
}

// Export spans using otelcol.exporter.otlp component

otelcol.exporter.otlp "tempo" {
    client {
        endpoint = "http://tempo:4317"

        // This is a local instance of Tempo, so we can skip TLS verification
        tls {
            insecure             = true
            insecure_skip_verify = true
        }
    }
}


// Section 6 - Application logs with Alloy

// Logs sent over HTTP from the applications to th loki.source.api
//  loki.source.api forward the logs to loki.process
// loki process extract the information like timestamp and adding labels and sent to loki.write
// series of stages during process time.
// Processed logs forward (Section:1 component) to loki using loki.write 

loki.source.api "mythical" {
    http {
        listen_address = "0.0.0.0"
        listen_port    = "3100"
    }
    forward_to = [loki.process.mythical.receiver]
}

loki.process "mythical" {
    stage.static_labels {
        values = {
           service = "mythical",
        }
    }
    stage.regex {
            expression=`^.*?loggedtime=(?P<loggedtime>\S+)`
    }

    stage.timestamp {
        source = "loggedtime"
        format = "2026-12-12T15:04:05.000Z07:00"
    }

    forward_to = [loki.write.mythical.receiver]
}

//Section 7
otelcol.connector.spanlogs "autologging" {
    roots = true
    spans = false
    processes = false

    span_attributes = ["http.method", "http.target", "http.status_code"]

    output {
        logs = [otelcol.exporter.loki.autologging.input]
    }
}

otelcol.exporter.loki "autologging" {
    forward_to = [loki.process.autologging.receiver]
}

loki.process "autologging" {
    stage.json {
       expressions = {"body" = ""}
    }

    stage.output {
       source = "body"
    }

    stage.logfmt {
        mapping = {
            http_method_extracted      = "http.method",
            http_status_code_extracted = "http.target",
            http_target_extracted      = "http.status_code",

        }
    }

    stage.labels {
        values = {
            method = "http_method_extracted",
            status = "http_status_code_extracted",
            target = "http_target_extracted",
        }
    }

    forward_to = [loki.write.mythical.receiver]
}